# spark-submit --packages org.mongodb.spark:mongo-spark-connector_2.12:10.3.0 spark_analysis.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, sum, min, max, desc, year, month, explode, split, lower, regexp_replace
from pyspark.sql.types import IntegerType, FloatType
import os

# Configuration
OUTPUT_DIR = "../../../04_Data_Visualization/4a_Web_Dashboard/static/data"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def run_analysis():
    print("\n" + "="*70)
    print("üöÄ ANALYSE BIG DATA AVEC SPARK")
    print("="*70)
    
    # Cr√©ation session Spark
    spark = SparkSession.builder \
        .appName("BigData_Scientific_Analysis_Complete") \
        .config("spark.master", "local[*]") \
        .config("spark.driver.memory", "4g") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    # ======================================
    # 1. CHARGEMENT DES DONN√âES
    # ======================================
    print("\nüìÇ 1. Chargement des donn√©es depuis MongoDB...")
    
    df = spark.read.format("mongodb") \
        .option("connection.uri", "mongodb://localhost:27017") \
        .option("database", "bigdata_project") \
        .option("collection", "raw_articles") \
        .load()
    
    print(f"   ‚úÖ {df.count()} articles charg√©s")
    
    # ======================================
    # 2. NETTOYAGE DES DONN√âES
    # ======================================
    print("\nüßπ 2. Nettoyage et transformation des donn√©es...")
    
    # Cast des types
    df_clean = df.withColumn("year", col("year").cast(IntegerType())) \
                 .withColumn("citations", col("citations").cast(IntegerType())) \
                 .filter(col("year").isNotNull()) \
                 .filter(col("country").isNotNull())
    
    print(f"   ‚úÖ {df_clean.count()} articles valides apr√®s nettoyage")
    
    # ======================================
    # 3. ANALYSES STATISTIQUES
    # ======================================
    print("\nüìä 3. Analyses statistiques globales...")
    
    # 3.1 Statistiques g√©n√©rales
    stats = df_clean.select(
        count("*").alias("total"),
        avg("citations").alias("avg_citations"),
        min("year").alias("min_year"),
        max("year").alias("max_year")
    ).collect()[0]
    
    print(f"\n   üìà STATISTIQUES GLOBALES:")
    print(f"      - Total publications : {stats['total']}")
    print(f"      - Moyenne citations : {stats['avg_citations']:.2f}" if stats['avg_citations'] else "N/A")
    print(f"      - P√©riode : {stats['min_year']} - {stats['max_year']}")
    
    # ======================================
    # 4. AGR√âGATIONS PAR DIMENSION
    # ======================================
    print("\nüìä 4. Agr√©gations par dimension...")
    
    # 4.1 Par ann√©e
    print("\n   üìÖ Par ann√©e...")
    year_analysis = df_clean.groupBy("year") \
        .agg(
            count("*").alias("count"),
            avg("citations").alias("avg_citations")
        ) \
        .orderBy("year")
    
    year_analysis.show()
    year_analysis.toPandas().to_json(
        f"{OUTPUT_DIR}/publications_by_year.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : publications_by_year.json")
    
    # 4.2 Par pays (Top 10)
    print("\n   üåç Par pays...")
    country_analysis = df_clean.groupBy("country") \
        .agg(count("*").alias("count")) \
        .orderBy(desc("count")) \
        .limit(10)
    
    country_analysis.show()
    country_analysis.toPandas().to_json(
        f"{OUTPUT_DIR}/publications_by_country.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : publications_by_country.json")
    
    # 4.3 Par keyword (th√©matique)
    print("\n   üîë Par keyword...")
    keyword_analysis = df_clean.groupBy("keyword") \
        .agg(count("*").alias("count")) \
        .orderBy(desc("count"))
    
    keyword_analysis.show()
    keyword_analysis.toPandas().to_json(
        f"{OUTPUT_DIR}/publications_by_keyword.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : publications_by_keyword.json")
    
    # 4.4 Par ville (Top 10)
    print("\n   üèôÔ∏è  Par ville...")
    city_analysis = df_clean.groupBy("city", "country") \
        .agg(count("*").alias("count")) \
        .orderBy(desc("count")) \
        .limit(10)
    
    city_analysis.show()
    city_analysis.toPandas().to_json(
        f"{OUTPUT_DIR}/publications_by_city.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : publications_by_city.json")
    
    # ======================================
    # 5. ANALYSE DES MOTS-CL√âS DANS LES TITRES
    # ======================================
    print("\nüî§ 5. Analyse des mots-cl√©s dans les titres...")
    
    # Extraction et comptage des mots
    words_df = df_clean.select(explode(split(lower(col("title")), " ")).alias("word")) \
        .filter(col("word").rlike("^[a-z]{4,}$"))  # Mots de 4+ lettres
    
    top_words = words_df.groupBy("word") \
        .agg(count("*").alias("count")) \
        .orderBy(desc("count")) \
        .limit(50)
    
    top_words.show(20)
    top_words.toPandas().to_json(
        f"{OUTPUT_DIR}/top_keywords.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : top_keywords.json")
    
    # ======================================
    # 6. ANALYSE CROIS√âE : PAYS √ó KEYWORD
    # ======================================
    print("\nüîÄ 6. Analyse crois√©e pays √ó keyword...")
    
    country_keyword = df_clean.groupBy("country", "keyword") \
        .agg(count("*").alias("count")) \
        .orderBy(desc("count")) \
        .limit(20)
    
    country_keyword.show()
    country_keyword.toPandas().to_json(
        f"{OUTPUT_DIR}/country_keyword_matrix.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : country_keyword_matrix.json")
    
    # ======================================
    # 7. √âVOLUTION TEMPORELLE PAR TH√âMATIQUE
    # ======================================
    print("\nüìà 7. √âvolution temporelle par th√©matique...")
    
    temporal_evolution = df_clean.groupBy("year", "keyword") \
        .agg(count("*").alias("count")) \
        .orderBy("year", "keyword")
    
    temporal_evolution.show(30)
    temporal_evolution.toPandas().to_json(
        f"{OUTPUT_DIR}/temporal_evolution.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : temporal_evolution.json")
    
    # ======================================
    # 8. QUARTILE DISTRIBUTION (Simulation)
    # ======================================
    print("\nüìä 8. Distribution par quartile (simul√©e)...")
    
    # Simulation des quartiles pour dashboard BI
    from pyspark.sql.functions import when, rand
    
    df_quartile = df_clean.withColumn(
        "quartile",
        when(rand() < 0.30, "Q1")
        .when(rand() < 0.55, "Q2")
        .when(rand() < 0.75, "Q3")
        .otherwise("Q4")
    )
    
    quartile_dist = df_quartile.groupBy("quartile") \
        .agg(count("*").alias("count")) \
        .orderBy("quartile")
    
    quartile_dist.show()
    quartile_dist.toPandas().to_json(
        f"{OUTPUT_DIR}/quartiles_distribution.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : quartiles_distribution.json")
    
    # ======================================
    # 9. TOP AUTEURS
    # ======================================
    print("\nüë• 9. Top 20 auteurs...")
    
    # Exploser le tableau d'auteurs pour compter par auteur
    authors_df = df_clean.select(explode(col("authors")).alias("author"))
    
    top_authors = authors_df.groupBy("author") \
        .agg(count("*").alias("publications")) \
        .orderBy(desc("publications")) \
        .limit(20)
    
    top_authors.show()
    top_authors.toPandas().to_json(
        f"{OUTPUT_DIR}/top_authors.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : top_authors.json")
    
    # ======================================
    # 10. TOP LABORATOIRES (par ville)
    # ======================================
    print("\nüèõÔ∏è 10. Top 20 laboratoires (par ville)...")
    
    # Utiliser ville comme proxy pour laboratoire
    from pyspark.sql.functions import concat, lit
    
    top_labs = df_clean.groupBy("city", "country") \
        .agg(count("*").alias("publications")) \
        .withColumn("laboratory", concat(col("city"), lit(" Research Center"))) \
        .select("laboratory", "publications") \
        .orderBy(desc("publications")) \
        .limit(20)
    
    top_labs.show()
    top_labs.toPandas().to_json(
        f"{OUTPUT_DIR}/top_laboratories.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : top_laboratories.json")
    
    # ======================================
    # 11. EXPORT DONN√âES GLOBALES (pour le dashboard)
    # ======================================
    print("\nüíæ 11. Export donn√©es globales pour le dashboard web...")
    
    global_export = df_clean.select(
        "title", "year", "country", "city", "keyword", "source", "authors", "abstract", "doi", "url"
    )
    
    global_export.toPandas().to_json(
        f"{OUTPUT_DIR}/global_data.json",
        orient="records"
    )
    print(f"   ‚úÖ Sauvegard√© : global_data.json")
    
    # ======================================
    # 9. R√âSUM√â FINAL
    # ======================================
    print("\n" + "="*70)
    print("‚úÖ ANALYSE TERMIN√âE AVEC SUCC√àS")
    print("="*70)
    print(f"\nüìÅ Fichiers g√©n√©r√©s dans : {OUTPUT_DIR}/")
    print("   1. publications_by_year.json")
    print("   2. publications_by_country.json")
    print("   3. publications_by_keyword.json")
    print("   4. publications_by_city.json")
    print("   5. top_keywords.json")
    print("   6. country_keyword_matrix.json")
    print("   7. temporal_evolution.json")
    print("   8. quartiles_distribution.json")
    print("   9. top_authors.json")
    print("   10. top_laboratories.json")
    print("   11. global_data.json")
    print("\nüéâ Vous pouvez maintenant lancer le dashboard Flask !")
    
    spark.stop()

if __name__ == "__main__":
    run_analysis()